# # üè™ E-commerce Data Platform en AWS

**Autor:** Ruben Martin  
**Versi√≥n:** 2.0.0  
**Fecha:** 2025-07-16  

## üìã Descripci√≥n del Proyecto

Plataforma de datos de comercio electr√≥nico construida en AWS siguiendo las mejores pr√°cticas del **AWS Well-Architected Framework**. Esta infraestructura permite capturar, procesar y analizar datos de e-commerce en tiempo real utilizando servicios nativos de AWS.

### üéØ Objetivos

- **Captura en tiempo real** de eventos de e-commerce
- **Procesamiento escalable** de grandes vol√∫menes de datos
- **Almacenamiento optimizado** con pol√≠ticas de lifecycle
- **An√°lisis avanzado** con capacidades de Business Intelligence
- **Seguridad robusta** con cifrado end-to-end
- **Monitoreo completo** con alertas proactivas

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ    ‚îÇ   Kinesis Data  ‚îÇ    ‚îÇ   Kinesis Data  ‚îÇ
‚îÇ                 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Streams      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Analytics     ‚îÇ
‚îÇ ‚Ä¢ Web Events    ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Mobile Apps   ‚îÇ    ‚îÇ ‚Ä¢ Real-time     ‚îÇ    ‚îÇ ‚Ä¢ SQL Queries   ‚îÇ
‚îÇ ‚Ä¢ APIs          ‚îÇ    ‚îÇ ‚Ä¢ Scalable      ‚îÇ    ‚îÇ ‚Ä¢ Transformations‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Amazon S3     ‚îÇ    ‚îÇ   Kinesis Data  ‚îÇ    ‚îÇ   Amazon        ‚îÇ
‚îÇ                 ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Firehose      ‚îÇ    ‚îÇ   Redshift      ‚îÇ
‚îÇ ‚Ä¢ Data Lake     ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Raw Data      ‚îÇ    ‚îÇ ‚Ä¢ Buffering     ‚îÇ    ‚îÇ ‚Ä¢ Data Warehouse‚îÇ
‚îÇ ‚Ä¢ Processed     ‚îÇ    ‚îÇ ‚Ä¢ Compression   ‚îÇ    ‚îÇ ‚Ä¢ Analytics     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîß Componentes Principales

| Servicio | Prop√≥sito | Caracter√≠sticas |
|----------|-----------|-----------------|
| **Amazon Kinesis Data Streams** | Ingesta de datos en tiempo real | Auto-scaling, retenci√≥n configurable |
| **Amazon Kinesis Data Analytics** | Procesamiento de stream | SQL en tiempo real, detecci√≥n de anomal√≠as |
| **Amazon Kinesis Data Firehose** | Entrega de datos | Compresi√≥n autom√°tica, buffering inteligente |
| **Amazon S3** | Data Lake | Lifecycle policies, versionado, cifrado |
| **Amazon Redshift** | Data Warehouse | Columnar, compresi√≥n, escalable |
| **AWS Lambda** | Procesamiento serverless | Enriquecimiento, validaci√≥n autom√°tica |
| **Amazon CloudWatch** | Monitoreo | M√©tricas, logs, alertas |

## üõ°Ô∏è Pilares del Well-Architected Framework

### 1. **Excelencia Operacional**
- ‚úÖ Infraestructura como c√≥digo con Terraform
- ‚úÖ Automatizaci√≥n completa del despliegue
- ‚úÖ Monitoreo proactivo con CloudWatch
- ‚úÖ Alertas configurables por entorno
- ‚úÖ Logging estructurado para troubleshooting

### 2. **Seguridad**
- ‚úÖ Cifrado en reposo con AWS KMS
- ‚úÖ Cifrado en tr√°nsito con TLS/SSL
- ‚úÖ VPC endpoints para comunicaci√≥n privada
- ‚úÖ IAM roles con principio de menor privilegio
- ‚úÖ Network segmentation con subnets privadas

### 3. **Confiabilidad**
- ‚úÖ Deployment multi-AZ
- ‚úÖ Backups autom√°ticos configurados
- ‚úÖ Recuperaci√≥n ante desastres
- ‚úÖ Auto-scaling basado en demanda
- ‚úÖ Health checks automatizados

### 4. **Eficiencia de Rendimiento**
- ‚úÖ Servicios nativos optimizados
- ‚úÖ Caching inteligente
- ‚úÖ Compresi√≥n autom√°tica de datos
- ‚úÖ Particionado eficiente en Redshift
- ‚úÖ Paralelizaci√≥n de procesamiento

### 5. **Optimizaci√≥n de Costos**
- ‚úÖ Pol√≠ticas de lifecycle en S3
- ‚úÖ Reserved Instances para Redshift
- ‚úÖ Spot Instances para desarrollo
- ‚úÖ Tagging para cost allocation
- ‚úÖ Monitoring de costos automatizado

### 6. **Sostenibilidad**
- ‚úÖ Recursos dimensionados por entorno
- ‚úÖ Auto-shutdown en desarrollo
- ‚úÖ Pol√≠ticas de retenci√≥n optimizadas
- ‚úÖ Servicios serverless cuando es posible

## üöÄ Inicio R√°pido

### Prerrequisitos

```bash
# Instalar Terraform
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install terraform

# Instalar AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# Configurar credenciales AWS
aws configure
```

### Despliegue

```bash
# 1. Clonar el repositorio
git clone https://github.com/FpuntoRubenM/ecommerce-data-platform.git
cd ecommerce-data-platform

# 2. Configurar variables
cp terraform.tfvars.example terraform.tfvars
# Editar terraform.tfvars con tus valores espec√≠ficos

# 3. Inicializar Terraform
terraform init

# 4. Planificar el despliegue
terraform plan -var="environment=dev"

# 5. Aplicar la infraestructura
terraform apply -var="environment=dev"
```

## ‚öôÔ∏è Configuraci√≥n por Entornos

### üî¨ Desarrollo (dev)
```hcl
environment = "dev"
kinesis_shard_count = 1
redshift_node_type = "dc2.large"
redshift_cluster_type = "single-node"
enable_detailed_monitoring = false
backup_retention_period = 3
```

### üß™ Staging
```hcl
environment = "staging"
kinesis_shard_count = 2
redshift_node_type = "dc2.large"
redshift_cluster_type = "single-node"
enable_detailed_monitoring = true
backup_retention_period = 7
```

### üè≠ Producci√≥n
```hcl
environment = "prod"
kinesis_shard_count = 4
redshift_node_type = "dc2.8xlarge"
redshift_cluster_type = "multi-node"
redshift_number_of_nodes = 2
enable_detailed_monitoring = true
backup_retention_period = 30
enable_deletion_protection = true
```

## üìä Estructura de Datos

### Eventos de E-commerce Soportados

#### üõí Transacciones
```json
{
  "event_type": "purchase",
  "timestamp": "2025-07-16T10:30:00Z",
  "transaction_id": "txn_12345",
  "customer_id": "cust_67890",
  "items": [
    {
      "product_id": "prod_111",
      "quantity": 2,
      "unit_price": 29.99,
      "category": "electronics"
    }
  ],
  "total_amount": 59.98,
  "currency": "USD",
  "payment_method": "credit_card",
  "shipping_address": {
    "country": "ES",
    "city": "Madrid",
    "postal_code": "28001"
  }
}
```

#### üë§ Eventos de Usuario
```json
{
  "event_type": "page_view",
  "timestamp": "2025-07-16T10:30:00Z",
  "user_id": "user_12345",
  "session_id": "sess_67890",
  "page_url": "/product/electronics/smartphone",
  "referrer": "https://google.com",
  "user_agent": "Mozilla/5.0...",
  "device_type": "mobile"
}
```

#### üì¶ Productos
```json
{
  "product_id": "prod_111",
  "name": "Smartphone Pro Max",
  "category": "electronics",
  "brand": "TechBrand",
  "price": 899.99,
  "currency": "USD",
  "stock_quantity": 150,
  "attributes": {
    "color": "black",
    "storage": "256GB",
    "screen_size": "6.7"
  },
  "created_at": "2025-07-16T10:30:00Z",
  "updated_at": "2025-07-16T10:30:00Z"
}
```

## üîç Monitoreo y Alertas

### M√©tricas Clave

| M√©trica | Umbral | Acci√≥n |
|---------|--------|--------|
| **Kinesis IncomingRecords** | > 1000/min | Scale up shards |
| **Firehose DeliveryErrors** | > 1% | Investigar y alertar |
| **Redshift CPU** | > 80% | Scale up cluster |
| **S3 Storage** | > 1TB | Revisar lifecycle policies |
| **Lambda Errors** | > 5% | Alertar al equipo |

### üìß Configuraci√≥n de Alertas

```bash
# Configurar email de notificaciones
aws sns create-topic --name ecommerce-alerts
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:123456789012:ecommerce-alerts \
  --protocol email \
  --notification-endpoint ruben.martin@tuempresa.com
```

## üíæ Backup y Recuperaci√≥n

### Estrategia de Backup

| Componente | Frecuencia | Retenci√≥n | Ubicaci√≥n |
|------------|------------|-----------|-----------|
| **Redshift** | Autom√°tico (24h) | 7-30 d√≠as | S3 cross-region |
| **S3 Data** | Continuo (versionado) | Seg√∫n lifecycle | Glacier/Deep Archive |
| **Configuraci√≥n** | Manual | Permanente | Git repository |

### üîÑ Procedimiento de Recuperaci√≥n

```bash
# Restaurar cluster Redshift
aws redshift restore-from-cluster-snapshot \
  --cluster-identifier ecommerce-dw-restored \
  --snapshot-identifier ecommerce-dw-snapshot-2025-07-16

# Verificar integridad de datos
psql -h redshift-cluster.region.redshift.amazonaws.com \
     -U admin -d ecommerce \
     -c "SELECT COUNT(*) FROM transactions WHERE date >= '2025-07-15';"
```

## üîê Seguridad

### Cifrado

- **En reposo:** AES-256 con AWS KMS
- **En tr√°nsito:** TLS 1.2+
- **Claves:** Rotaci√≥n autom√°tica anual

### Control de Acceso

```bash
# Crear usuario desarrollador
aws iam create-user --user-name developer-juan
aws iam add-user-to-group --user-name developer-juan --group-name ecommerce-data-platform-dev-developers

# Generar credenciales de acceso
aws iam create-access-key --user-name developer-juan
```

### Auditor√≠a

- **CloudTrail:** Registro de todas las API calls
- **VPC Flow Logs:** Monitoreo de tr√°fico de red
- **CloudWatch Logs:** Logs de aplicaciones centralizados

## üîß Operaciones

### Comandos √ötiles

```bash
# Ver estado de la infraestructura
terraform show

# Verificar configuraci√≥n
terraform validate

# Planificar cambios
terraform plan -var="environment=prod"

# Aplicar cambios espec√≠ficos
terraform apply -target=module.kinesis

# Ver outputs importantes
terraform output

# Destruir entorno de desarrollo
terraform destroy -var="environment=dev" -auto-approve
```

### üìà Escalamiento

#### Kinesis Data Streams
```bash
# Aumentar shards manualmente
aws kinesis update-shard-count \
  --stream-name ecommerce-data-platform-prod-ecommerce-events \
  --target-shard-count 8 \
  --scaling-type UNIFORM_SCALING
```

#### Redshift
```bash
# Resize cluster
aws redshift modify-cluster \
  --cluster-identifier ecommerce-data-platform-prod-ecommerce-dw \
  --node-type dc2.8xlarge \
  --number-of-nodes 4
```

### üîç Troubleshooting

#### Problemas Comunes

**1. Kinesis Throttling**
```bash
# Verificar m√©tricas
aws cloudwatch get-metric-statistics \
  --namespace AWS/Kinesis \
  --metric-name IncomingRecords \
  --dimensions Name=StreamName,Value=ecommerce-events \
  --start-time 2025-07-16T09:00:00Z \
  --end-time 2025-07-16T10:00:00Z \
  --period 300 \
  --statistics Sum
```

**2. Firehose Delivery Errors**
```bash
# Revisar logs de error
aws logs filter-log-events \
  --log-group-name /aws/kinesisfirehose/ecommerce-data-firehose \
  --filter-pattern "ERROR"
```

**3. Redshift Connection Issues**
```bash
# Verificar security group
aws ec2 describe-security-groups \
  --filters "Name=group-name,Values=*redshift*"

# Test de conectividad
telnet redshift-cluster.region.redshift.amazonaws.com 5439
```

## üìä An√°lisis de Datos

### Consultas SQL de Ejemplo

#### An√°lisis de Ventas
```sql
-- Top 10 productos m√°s vendidos
SELECT 
    p.product_name,
    SUM(t.quantity) as total_quantity,
    SUM(t.total_amount) as revenue
FROM transactions t
JOIN products p ON t.product_id = p.product_id
WHERE t.transaction_date >= '2025-07-01'
GROUP BY p.product_name
ORDER BY revenue DESC
LIMIT 10;
```

#### An√°lisis de Comportamiento
```sql
-- Funnel de conversi√≥n por hora
SELECT 
    EXTRACT(hour FROM timestamp) as hour,
    COUNT(CASE WHEN event_type = 'page_view' THEN 1 END) as page_views,
    COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) as add_to_cart,
    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) as purchases,
    ROUND(
        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) * 100.0 / 
        NULLIF(COUNT(CASE WHEN event_type = 'page_view' THEN 1 END), 0), 2
    ) as conversion_rate
FROM events
WHERE DATE(timestamp) = '2025-07-16'
GROUP BY hour
ORDER BY hour;
```

#### Segmentaci√≥n de Clientes
```sql
-- RFM Analysis
WITH customer_metrics AS (
    SELECT 
        customer_id,
        MAX(transaction_date) as last_purchase,
        COUNT(*) as frequency,
        SUM(total_amount) as monetary
    FROM transactions
    WHERE transaction_date >= '2025-01-01'
    GROUP BY customer_id
),
rfm_scores AS (
    SELECT 
        customer_id,
        NTILE(5) OVER (ORDER BY last_purchase DESC) as recency_score,
        NTILE(5) OVER (ORDER BY frequency) as frequency_score,
        NTILE(5) OVER (ORDER BY monetary) as monetary_score
    FROM customer_metrics
)
SELECT 
    customer_id,
    CASE 
        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'
        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'
        WHEN recency_score >= 3 AND frequency_score <= 2 THEN 'Potential Loyalists'
        WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'
        ELSE 'Others'
    END as customer_segment
FROM rfm_scores;
```

## üß™ Testing

### Tests de Infraestructura

```bash
# Validar configuraci√≥n Terraform
terraform validate

# Formato de c√≥digo
terraform fmt -recursive

# Linting con tflint
tflint --init
tflint

# Security scanning con Checkov
checkov -f main.tf
```

### Tests de Datos

```python
# test_data_quality.py - Ruben Martin
import boto3
import json
import pytest
from datetime import datetime

def test_kinesis_stream_exists():
    """Verificar que el stream de Kinesis existe y est√° activo"""
    kinesis = boto3.client('kinesis', region_name='us-east-1')
    
    response = kinesis.describe_stream(
        StreamName='ecommerce-data-platform-dev-ecommerce-events'
    )
    
    assert response['StreamDescription']['StreamStatus'] == 'ACTIVE'
    assert response['StreamDescription']['Shards']

def test_s3_bucket_encryption():
    """Verificar que el bucket S3 tiene cifrado habilitado"""
    s3 = boto3.client('s3')
    
    response = s3.get_bucket_encryption(
        Bucket='ecommerce-data-platform-dev-datalake-12345678'
    )
    
    assert 'ServerSideEncryptionConfiguration' in response
    rules = response['ServerSideEncryptionConfiguration']['Rules']
    assert any(rule['ApplyServerSideEncryptionByDefault']['SSEAlgorithm'] == 'aws:kms' 
              for rule in rules)

def test_redshift_cluster_health():
    """Verificar que el cluster Redshift est√° disponible"""
    redshift = boto3.client('redshift')
    
    response = redshift.describe_clusters(
        ClusterIdentifier='ecommerce-data-platform-dev-ecommerce-dw'
    )
    
    cluster = response['Clusters'][0]
    assert cluster['ClusterStatus'] == 'available'
    assert cluster['Encrypted'] == True
```

### Tests de Pipeline

```python
# test_data_pipeline.py - Ruben Martin
import json
import boto3
from moto import mock_kinesis, mock_s3

@mock_kinesis
@mock_s3
def test_end_to_end_pipeline():
    """Test completo del pipeline de datos"""
    # Setup
    kinesis = boto3.client('kinesis', region_name='us-east-1')
    s3 = boto3.client('s3', region_name='us-east-1')
    
    # Crear recursos mock
    kinesis.create_stream(StreamName='test-stream', ShardCount=1)
    s3.create_bucket(Bucket='test-bucket')
    
    # Datos de prueba
    test_event = {
        "event_type": "purchase",
        "timestamp": datetime.utcnow().isoformat(),
        "customer_id": "test_customer_123",
        "amount": 99.99
    }
    
    # Enviar datos a Kinesis
    response = kinesis.put_record(
        StreamName='test-stream',
        Data=json.dumps(test_event),
        PartitionKey='test_customer_123'
    )
    
    assert response['ResponseMetadata']['HTTPStatusCode'] == 200
```

## üì± APIs y SDKs

### API de Ingesta

```python
# ingest_api.py - Ruben Martin
import boto3
import json
from datetime import datetime

class EcommerceDataIngester:
    """
    Cliente para enviar datos al pipeline de e-commerce
    Autor: Ruben Martin
    """
    
    def __init__(self, stream_name, region='us-east-1'):
        self.kinesis = boto3.client('kinesis', region_name=region)
        self.stream_name = stream_name
    
    def send_transaction(self, transaction_data):
        """Enviar datos de transacci√≥n"""
        # Validar datos requeridos
        required_fields = ['customer_id', 'amount', 'currency']
        for field in required_fields:
            if field not in transaction_data:
                raise ValueError(f"Campo requerido faltante: {field}")
        
        # Enriquecer con timestamp
        transaction_data['timestamp'] = datetime.utcnow().isoformat()
        transaction_data['event_type'] = 'transaction'
        
        # Enviar a Kinesis
        response = self.kinesis.put_record(
            StreamName=self.stream_name,
            Data=json.dumps(transaction_data),
            PartitionKey=transaction_data['customer_id']
        )
        
        return response['SequenceNumber']
    
    def send_user_event(self, event_data):
        """Enviar evento de usuario"""
        event_data['timestamp'] = datetime.utcnow().isoformat()
        
        response = self.kinesis.put_record(
            StreamName=self.stream_name,
            Data=json.dumps(event_data),
            PartitionKey=event_data.get('user_id', 'anonymous')
        )
        
        return response['SequenceNumber']

# Ejemplo de uso
ingester = EcommerceDataIngester('ecommerce-data-platform-prod-ecommerce-events')

# Enviar transacci√≥n
transaction = {
    'customer_id': 'cust_12345',
    'amount': 129.99,
    'currency': 'USD',
    'items': [
        {'product_id': 'prod_567', 'quantity': 1, 'price': 129.99}
    ]
}
sequence_number = ingester.send_transaction(transaction)
print(f"Transacci√≥n enviada: {sequence_number}")
```

### SDK JavaScript

```javascript
// ecommerce-analytics.js - Ruben Martin
class EcommerceAnalytics {
    constructor(config) {
        this.apiGatewayUrl = config.apiGatewayUrl;
        this.apiKey = config.apiKey;
    }
    
    // Rastrear evento de p√°gina
    trackPageView(pageData) {
        return this.sendEvent('page_view', {
            page_url: pageData.url,
            referrer: document.referrer,
            user_agent: navigator.userAgent,
            timestamp: new Date().toISOString()
        });
    }
    
    // Rastrear evento de producto
    trackProductView(productData) {
        return this.sendEvent('product_view', {
            product_id: productData.id,
            product_name: productData.name,
            category: productData.category,
            price: productData.price
        });
    }
    
    // Rastrear transacci√≥n
    trackPurchase(transactionData) {
        return this.sendEvent('purchase', transactionData);
    }
    
    async sendEvent(eventType, data) {
        const payload = {
            event_type: eventType,
            ...data,
            session_id: this.getSessionId(),
            user_id: this.getUserId()
        };
        
        try {
            const response = await fetch(`${this.apiGatewayUrl}/events`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'X-API-Key': this.apiKey
                },
                body: JSON.stringify(payload)
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            return await response.json();
        } catch (error) {
            console.error('Error enviando evento:', error);
            throw error;
        }
    }
    
    getSessionId() {
        // Implementar l√≥gica de sesi√≥n
        return sessionStorage.getItem('session_id') || 'anonymous';
    }
    
    getUserId() {
        // Implementar l√≥gica de usuario
        return localStorage.getItem('user_id') || null;
    }
}

// Uso
const analytics = new EcommerceAnalytics({
    apiGatewayUrl: 'https://api.tudominio.com',
    apiKey: 'tu-api-key'
});

// Rastrear autom√°ticamente page views
analytics.trackPageView({ url: window.location.href });
```

## üöÄ CI/CD Pipeline

### GitHub Actions Workflow

```yaml
# .github/workflows/deploy.yml - Ruben Martin
name: Deploy E-commerce Data Platform

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  TF_VERSION: 1.5.0
  AWS_REGION: us-east-1

jobs:
  validate:
    name: Validate Terraform
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Format Check
        run: terraform fmt -check -recursive
      
      - name: Terraform Init
        run: terraform init -backend=false
      
      - name: Terraform Validate
        run: terraform validate
      
      - name: Run Security Scan
        uses: bridgecrewio/checkov-action@master
        with:
          directory: .
          framework: terraform

  deploy-dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: validate
    if: github.ref == 'refs/heads/develop'
    environment: development
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Init
        run: terraform init
      
      - name: Terraform Plan
        run: terraform plan -var="environment=dev" -out=tfplan
      
      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
      
      - name: Run Integration Tests
        run: |
          pip install pytest boto3 moto
          pytest tests/ -v

  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: validate
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Init
        run: terraform init
      
      - name: Terraform Plan
        run: terraform plan -var="environment=prod" -out=tfplan
      
      - name: Manual Approval
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: ruben-martin
          minimum-approvals: 1
          issue-title: "Deploy to Production"
      
      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
      
      - name: Notify Success
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: "üöÄ Production deployment successful!"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

## üìã Checklist de Despliegue

### Pre-despliegue
- [ ] ‚úÖ Credenciales AWS configuradas
- [ ] ‚úÖ Variables de entorno definidas
- [ ] ‚úÖ Permisos IAM verificados
- [ ] ‚úÖ L√≠mites de servicio revisados
- [ ] ‚úÖ Backup de configuraci√≥n existente

### Despliegue
- [ ] ‚úÖ `terraform validate` exitoso
- [ ] ‚úÖ `terraform plan` revisado
- [ ] ‚úÖ Tests de seguridad pasados
- [ ] ‚úÖ `terraform apply` ejecutado
- [ ] ‚úÖ Outputs verificados

### Post-despliegue
- [ ] ‚úÖ Health checks pasados
- [ ] ‚úÖ M√©tricas en CloudWatch operativas
- [ ] ‚úÖ Alertas configuradas
- [ ] ‚úÖ Tests de integraci√≥n exitosos
- [ ] ‚úÖ Documentaci√≥n actualizada

## ü§ù Contribuciones

### Gu√≠a para Desarrolladores

1. **Fork del repositorio**
2. **Crear rama feature:** `git checkout -b feature/nueva-funcionalidad`
3. **Commitear cambios:** `git commit -am 'Agregar nueva funcionalidad'`
4. **Push a la rama:** `git push origin feature/nueva-funcionalidad`
5. **Crear Pull Request**

### Est√°ndares de C√≥digo

- **Terraform:** Seguir [est√°ndares de HashiCorp](https://www.terraform.io/docs/language/syntax/style.html)
- **Python:** PEP 8 compliance
- **SQL:** May√∫sculas para keywords, snake_case para nombres
- **Documentaci√≥n:** Comentarios en espa√±ol, autor claramente identificado

## üìû Soporte

### Contacto

**üë®‚Äçüíª Autor:** Ruben Martin  
**üìß Email:** ruben.martin@tuempresa.com  
**üîó LinkedIn:** [Ruben Martin](https://linkedin.com/in/ruben-martin)  

### Issues Conocidos

| Issue | Descripci√≥n | Workaround | Status |
|-------|-------------|------------|--------|
| #001 | Latencia en Kinesis Analytics | Aumentar memory allocation | ‚úÖ Resuelto |
| #002 | Timeout en Lambda para archivos grandes | Implementar procesamiento por chunks | üîÑ En progreso |

### FAQ

**Q: ¬øC√≥mo cambio el n√∫mero de shards en Kinesis?**  
A: Modifica la variable `kinesis_shard_count` en `terraform.tfvars` y ejecuta `terraform apply`.

**Q: ¬øPuedo usar mi propia clave KMS?**  
A: S√≠, modifica el m√≥dulo KMS para usar una clave existente en lugar de crear una nueva.

**Q: ¬øC√≥mo escalo el cluster Redshift?**  
A: Actualiza las variables `redshift_node_type` y `redshift_number_of_nodes` y aplica los cambios.

---

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## üôè Agradecimientos

- **AWS Well-Architected Framework** por las mejores pr√°cticas
- **Terraform Community** por los m√≥dulos y ejemplos
- **Equipo de Data Engineering** por el feedback y testing

---

**üìÖ √öltima actualizaci√≥n:** 2025-07-16  
**‚ú® Creado con ‚ù§Ô∏è por Ruben Martin** "README.md"
